{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-14T23:45:09.574981Z",
     "start_time": "2025-04-14T23:45:07.802259Z"
    }
   },
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "from datasets import load_dataset\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Authentification à Hugging Face (nécessaire pour accéder aux datasets)\n",
    "# Vous devrez avoir généré un token sur https://huggingface.co/settings/tokens\n",
    "# et exécuté `huggingface-cli login` en ligne de commande avant de lancer ce script\n",
    "# ou utiliser la méthode ci-dessous avec votre token\n",
    "\n",
    "# Décommentez et ajoutez votre token si vous n'avez pas fait login via CLI\n",
    "# login(token=\"votre_token_huggingface\")\n",
    "\n",
    "def load_and_prepare_datasets():\n",
    "    \"\"\"\n",
    "    Charge les deux datasets d'ALOHA et les concatène en ajoutant des tags\n",
    "    pour distinguer les actions.\n",
    "    \"\"\"\n",
    "    print(\"Chargement du dataset pour la tâche de transfert...\")\n",
    "    ds_transfer = load_dataset(\"lerobot/aloha_sim_transfer_cube_human\")\n",
    "    \n",
    "    print(\"Chargement du dataset pour la tâche d'insertion...\")\n",
    "    ds_insertion = load_dataset(\"lerobot/aloha_sim_insertion_human\")\n",
    "    \n",
    "    # Affichage des informations sur les datasets\n",
    "    print(f\"Dataset transfert: {ds_transfer}\")\n",
    "    print(f\"Dataset insertion: {ds_insertion}\")\n",
    "    \n",
    "    # Récupération des échantillons pour examiner la structure\n",
    "    transfer_example = ds_transfer[\"train\"][0] if \"train\" in ds_transfer else ds_transfer[next(iter(ds_transfer))][0]\n",
    "    insertion_example = ds_insertion[\"train\"][0] if \"train\" in ds_insertion else ds_insertion[next(iter(ds_insertion))][0]\n",
    "    \n",
    "    print(\"\\nExemple d'échantillon de transfert:\")\n",
    "    for key in transfer_example:\n",
    "        if isinstance(transfer_example[key], (int, float, str, bool)):\n",
    "            print(f\"{key}: {transfer_example[key]}\")\n",
    "        else:\n",
    "            print(f\"{key}: Type {type(transfer_example[key])}, Forme {np.array(transfer_example[key]).shape if hasattr(transfer_example[key], '__len__') else 'scalaire'}\")\n",
    "    \n",
    "    print(\"\\nExemple d'échantillon d'insertion:\")\n",
    "    for key in insertion_example:\n",
    "        if isinstance(insertion_example[key], (int, float, str, bool)):\n",
    "            print(f\"{key}: {insertion_example[key]}\")\n",
    "        else:\n",
    "            print(f\"{key}: Type {type(insertion_example[key])}, Forme {np.array(insertion_example[key]).shape if hasattr(insertion_example[key], '__len__') else 'scalaire'}\")\n",
    "    \n",
    "    # Fonction pour ajouter un tag à chaque échantillon\n",
    "    def add_task_tag_to_dataset(dataset, task_tag):\n",
    "        \"\"\"Ajoute un tag de tâche à chaque échantillon du dataset.\"\"\"\n",
    "        def add_tag(example):\n",
    "            example[\"task_tag\"] = task_tag\n",
    "            return example\n",
    "        \n",
    "        # Applique la fonction à chaque split du dataset\n",
    "        tagged_dataset = {}\n",
    "        for split in dataset:\n",
    "            tagged_dataset[split] = dataset[split].map(add_tag)\n",
    "        \n",
    "        return tagged_dataset\n",
    "    \n",
    "    # Ajouter les tags aux datasets\n",
    "    print(\"\\nAjout des tags aux datasets...\")\n",
    "    tagged_transfer = add_task_tag_to_dataset(ds_transfer, \"transfer\")\n",
    "    tagged_insertion = add_task_tag_to_dataset(ds_insertion, \"insertion\")\n",
    "    \n",
    "    # Concaténer les datasets\n",
    "    print(\"Concaténation des datasets...\")\n",
    "    combined_dataset = {}\n",
    "    \n",
    "    # Pour chaque split présent dans au moins l'un des datasets\n",
    "    all_splits = set(tagged_transfer.keys()).union(set(tagged_insertion.keys()))\n",
    "    for split in all_splits:\n",
    "        if split in tagged_transfer and split in tagged_insertion:\n",
    "            # Si le split existe dans les deux datasets, les concaténer\n",
    "            combined_dataset[split] = concatenate_datasets([tagged_transfer[split], tagged_insertion[split]])\n",
    "        elif split in tagged_transfer:\n",
    "            # Si le split n'existe que dans le dataset de transfert\n",
    "            combined_dataset[split] = tagged_transfer[split]\n",
    "        else:\n",
    "            # Si le split n'existe que dans le dataset d'insertion\n",
    "            combined_dataset[split] = tagged_insertion[split]\n",
    "    \n",
    "    # Afficher les informations sur le dataset combiné\n",
    "    print(\"\\nDataset combiné:\")\n",
    "    for split in combined_dataset:\n",
    "        print(f\"{split}: {len(combined_dataset[split])} échantillons\")\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "# Exécution de la fonction de préparation des datasets\n",
    "if __name__ == \"__main__\":\n",
    "    combined_dataset = load_and_prepare_datasets()\n",
    "    \n",
    "    # Exemple d'accès aux données du dataset combiné\n",
    "    if \"train\" in combined_dataset:\n",
    "        train_split = combined_dataset[\"train\"]\n",
    "        \n",
    "        # Comptage des échantillons par type de tâche\n",
    "        task_counts = train_split.to_pandas()[\"task_tag\"].value_counts()\n",
    "        print(\"\\nRépartition des tâches dans le split 'train':\")\n",
    "        print(task_counts)\n",
    "        \n",
    "        # Échantillon aléatoire pour vérification\n",
    "        print(\"\\nExemple d'échantillon du dataset combiné:\")\n",
    "        random_sample = train_split[np.random.randint(0, len(train_split))]\n",
    "        print(f\"Tâche: {random_sample['task_tag']}\")\n",
    "        for key in random_sample:\n",
    "            if key != \"task_tag\" and isinstance(random_sample[key], (int, float, str, bool)):\n",
    "                print(f\"{key}: {random_sample[key]}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset pour la tâche de transfert...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9380c0f3f63041c38481a43a38b3d0b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16568aa53dc84bf0b1361425af515363"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset pour la tâche d'insertion...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "abcb15cdbf244280be8711035e6ad44d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49a9e269f01c40c8abaf9e76f3f6deed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset transfert: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "Dataset insertion: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "\n",
      "Exemple d'échantillon de transfert:\n",
      "observation.state: Type <class 'list'>, Forme (14,)\n",
      "action: Type <class 'list'>, Forme (14,)\n",
      "episode_index: 0\n",
      "frame_index: 0\n",
      "timestamp: 0.0\n",
      "next.done: False\n",
      "index: 0\n",
      "task_index: 0\n",
      "\n",
      "Exemple d'échantillon d'insertion:\n",
      "observation.state: Type <class 'list'>, Forme (14,)\n",
      "action: Type <class 'list'>, Forme (14,)\n",
      "episode_index: 0\n",
      "frame_index: 0\n",
      "timestamp: 0.0\n",
      "next.done: False\n",
      "index: 0\n",
      "task_index: 0\n",
      "\n",
      "Ajout des tags aux datasets...\n",
      "Concaténation des datasets...\n",
      "\n",
      "Dataset combiné:\n",
      "train: 45000 échantillons\n",
      "\n",
      "Répartition des tâches dans le split 'train':\n",
      "task_tag\n",
      "insertion    25000\n",
      "transfer     20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Exemple d'échantillon du dataset combiné:\n",
      "Tâche: transfer\n",
      "episode_index: 41\n",
      "frame_index: 357\n",
      "timestamp: 7.139999866485596\n",
      "next.done: False\n",
      "index: 16757\n",
      "task_index: 0\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T23:47:53.740427Z",
     "start_time": "2025-04-14T23:47:52.425963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "from transformers import AutoModel, AutoConfig, PreTrainedModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb  # Pour le suivi des expériences (optionnel)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction de chargement des modèles pré-entraînés pour les deux tâches\n",
    "def load_pretrained_models():\n",
    "    \"\"\"\n",
    "    Charge les deux modèles pré-entraînés: un pour le transfert et un pour l'insertion\n",
    "    à partir des chemins locaux.\n",
    "    \"\"\"\n",
    "    print(\"Chargement du modèle pour la tâche de transfert...\")\n",
    "    transfer_model_path = \"/Users/louloute/PycharmProjects/INF8225_projet/Models/transfer/model.safetensors\"\n",
    "    transfer_model = AutoModel.from_pretrained(\n",
    "        os.path.dirname(transfer_model_path),\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    print(\"Chargement du modèle pour la tâche d'insertion...\")\n",
    "    insertion_model_path = \"/Users/louloute/PycharmProjects/INF8225_projet/Models/insertion/model.safetensors\"\n",
    "    insertion_model = AutoModel.from_pretrained(\n",
    "        os.path.dirname(insertion_model_path),\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # Afficher l'architecture des modèles\n",
    "    print(\"\\nArchitecture du modèle de transfert:\")\n",
    "    print(transfer_model)\n",
    "    \n",
    "    print(\"\\nArchitecture du modèle d'insertion:\")\n",
    "    print(insertion_model)\n",
    "    \n",
    "    # Examiner les paramètres des modèles\n",
    "    transfer_params = sum(p.numel() for p in transfer_model.parameters())\n",
    "    insertion_params = sum(p.numel() for p in insertion_model.parameters())\n",
    "    \n",
    "    print(f\"\\nNombre de paramètres du modèle de transfert: {transfer_params:,}\")\n",
    "    print(f\"Nombre de paramètres du modèle d'insertion: {insertion_params:,}\")\n",
    "    \n",
    "    return transfer_model, insertion_model\n",
    "# Classe pour le dataset PyTorch à partir du dataset Hugging Face\n",
    "class ALOHADataset(Dataset):\n",
    "    def __init__(self, hf_dataset, task_specific=False, task=None):\n",
    "        \"\"\"\n",
    "        Initialise un dataset PyTorch à partir d'un dataset Hugging Face.\n",
    "        \n",
    "        Args:\n",
    "            hf_dataset: Le dataset Hugging Face\n",
    "            task_specific: Si True, filtre le dataset pour une tâche spécifique\n",
    "            task: La tâche à filtrer ('transfer' ou 'insertion')\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        \n",
    "        # Si on veut uniquement les données d'une tâche spécifique\n",
    "        if task_specific and task:\n",
    "            indices = [i for i, sample in enumerate(hf_dataset) if sample[\"task_tag\"] == task]\n",
    "            self.indices = indices\n",
    "        else:\n",
    "            self.indices = list(range(len(hf_dataset)))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.hf_dataset[self.indices[idx]]\n",
    "        # Adapter les champs en fonction de la structure réelle des données\n",
    "        # Ici on suppose que les features principales sont dans 'input_features'\n",
    "        # et les labels dans 'labels'\n",
    "        # À adapter selon la structure réelle des datasets ALOHA\n",
    "        \n",
    "        # Récupérer les caractéristiques d'entrée\n",
    "        if \"input_features\" in sample:\n",
    "            features = torch.tensor(sample[\"input_features\"], dtype=torch.float32)\n",
    "        else:\n",
    "            # Si la structure est différente, on crée un tensor à partir des features disponibles\n",
    "            # Ceci est un exemple - à adapter selon les données réelles\n",
    "            features = []\n",
    "            for key in sample:\n",
    "                if key not in [\"task_tag\", \"labels\"] and isinstance(sample[key], (list, np.ndarray)):\n",
    "                    features.append(torch.tensor(sample[key], dtype=torch.float32))\n",
    "            \n",
    "            if features:\n",
    "                features = torch.cat(features, dim=0)\n",
    "            else:\n",
    "                # Si aucune feature n'est trouvée, créer un tensor vide\n",
    "                features = torch.tensor([], dtype=torch.float32)\n",
    "        \n",
    "        # Récupérer les labels\n",
    "        if \"labels\" in sample:\n",
    "            labels = torch.tensor(sample[\"labels\"], dtype=torch.float32)\n",
    "        else:\n",
    "            # Si aucun label explicite, utiliser une valeur par défaut\n",
    "            labels = torch.tensor([0.0], dtype=torch.float32)\n",
    "        \n",
    "        # Récupérer le tag de tâche et le convertir en entier (0 pour transfer, 1 pour insertion)\n",
    "        task_id = 0 if sample[\"task_tag\"] == \"transfer\" else 1\n",
    "        task_tensor = torch.tensor([task_id], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"features\": features,\n",
    "            \"labels\": labels,\n",
    "            \"task_id\": task_tensor\n",
    "        }\n",
    "\n",
    "# Classe du modèle unifié\n",
    "class UnifiedALOHAModel(nn.Module):\n",
    "    def __init__(self, transfer_model, insertion_model):\n",
    "        \"\"\"\n",
    "        Crée un modèle unifié à partir des deux modèles pré-entraînés.\n",
    "        \n",
    "        Args:\n",
    "            transfer_model: Modèle pré-entraîné pour la tâche de transfert\n",
    "            insertion_model: Modèle pré-entraîné pour la tâche d'insertion\n",
    "        \"\"\"\n",
    "        super(UnifiedALOHAModel, self).__init__()\n",
    "        \n",
    "        # Sauvegarder les modèles pré-entraînés\n",
    "        self.transfer_model = transfer_model\n",
    "        self.insertion_model = insertion_model\n",
    "        \n",
    "        # Geler les paramètres des modèles pré-entraînés\n",
    "        # pour éviter de les modifier pendant l'entraînement initial\n",
    "        for param in self.transfer_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.insertion_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Obtenir la taille de sortie des modèles\n",
    "        # À adapter selon l'architecture réelle des modèles\n",
    "        try:\n",
    "            transfer_output_size = self.transfer_model.config.hidden_size\n",
    "        except:\n",
    "            # Si le modèle n'a pas d'attribut config.hidden_size, utiliser une valeur par défaut\n",
    "            transfer_output_size = 768  # Valeur courante pour les modèles transformers\n",
    "        \n",
    "        try:\n",
    "            insertion_output_size = self.insertion_model.config.hidden_size\n",
    "        except:\n",
    "            # Si le modèle n'a pas d'attribut config.hidden_size, utiliser une valeur par défaut\n",
    "            insertion_output_size = 768  # Valeur courante pour les modèles transformers\n",
    "        \n",
    "        # Couche de sélection de tâche\n",
    "        self.task_embedding = nn.Embedding(2, 64)  # 2 tâches, embedding de dimension 64\n",
    "        \n",
    "        # Couche de fusion qui prend la sortie du modèle et l'embedding de tâche\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(transfer_output_size + insertion_output_size + 64, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Couche de sortie (à adapter selon le type de sortie attendu)\n",
    "        self.output_layer = nn.Linear(256, transfer_output_size)  # On suppose que les deux modèles ont la même dimension de sortie\n",
    "        \n",
    "        # Layer norm pour normaliser les sorties\n",
    "        self.layer_norm = nn.LayerNorm(transfer_output_size)\n",
    "    \n",
    "    def forward(self, features, task_id):\n",
    "        \"\"\"\n",
    "        Propagation avant du modèle unifié.\n",
    "        \n",
    "        Args:\n",
    "            features: Les caractéristiques d'entrée\n",
    "            task_id: L'identifiant de la tâche (0 pour transfer, 1 pour insertion)\n",
    "        \n",
    "        Returns:\n",
    "            Les prédictions du modèle\n",
    "        \"\"\"\n",
    "        # Obtenir les embeddings de la tâche\n",
    "        task_emb = self.task_embedding(task_id).squeeze(1)\n",
    "        \n",
    "        # Passer les features dans les deux modèles pré-entraînés\n",
    "        with torch.no_grad():  # Pas besoin de calculer les gradients pour les modèles gelés\n",
    "            transfer_output = self.transfer_model(features)\n",
    "            insertion_output = self.insertion_model(features)\n",
    "        \n",
    "        # Extraire les sorties des modèles (à adapter selon la structure réelle des sorties)\n",
    "        if isinstance(transfer_output, tuple):\n",
    "            transfer_output = transfer_output[0]  # Prendre le premier élément si c'est un tuple\n",
    "        \n",
    "        if isinstance(insertion_output, tuple):\n",
    "            insertion_output = insertion_output[0]  # Prendre le premier élément si c'est un tuple\n",
    "        \n",
    "        # Concaténer les sorties des deux modèles et l'embedding de tâche\n",
    "        combined = torch.cat([transfer_output, insertion_output, task_emb], dim=1)\n",
    "        \n",
    "        # Passer dans la couche de fusion\n",
    "        fused = self.fusion_layer(combined)\n",
    "        \n",
    "        # Couche de sortie\n",
    "        output = self.output_layer(fused)\n",
    "        \n",
    "        # Normaliser la sortie\n",
    "        output = self.layer_norm(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Fonction pour l'entraînement du modèle unifié\n",
    "def train_unified_model(unified_model, train_dataloader, val_dataloader, \n",
    "                        num_epochs=10, learning_rate=1e-4, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle unifié.\n",
    "    \n",
    "    Args:\n",
    "        unified_model: Le modèle unifié\n",
    "        train_dataloader: DataLoader pour les données d'entraînement\n",
    "        val_dataloader: DataLoader pour les données de validation\n",
    "        num_epochs: Nombre d'époques d'entraînement\n",
    "        learning_rate: Taux d'apprentissage\n",
    "        device: Appareil sur lequel effectuer l'entraînement ('cuda' ou 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Le modèle entraîné et les historiques de perte\n",
    "    \"\"\"\n",
    "    # Déplacer le modèle sur l'appareil approprié\n",
    "    unified_model = unified_model.to(device)\n",
    "    \n",
    "    # Définir la fonction de perte et l'optimiseur\n",
    "    criterion = nn.MSELoss()  # À adapter selon la tâche (régression ou classification)\n",
    "    optimizer = torch.optim.Adam(unified_model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Historiques de perte\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Boucle d'entraînement\n",
    "    for epoch in range(num_epochs):\n",
    "        # Mode entraînement\n",
    "        unified_model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        # Boucle sur les batches d'entraînement\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Époque {epoch+1}/{num_epochs} (entraînement)\"):\n",
    "            # Déplacer les données sur l'appareil approprié\n",
    "            features = batch[\"features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            task_id = batch[\"task_id\"].to(device)\n",
    "            \n",
    "            # Réinitialiser les gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Propagation avant\n",
    "            outputs = unified_model(features, task_id)\n",
    "            \n",
    "            # Calculer la perte\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Rétropropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Mettre à jour les poids\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumuler la perte\n",
    "            epoch_train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        \n",
    "        # Calculer la perte moyenne pour l'époque\n",
    "        epoch_train_loss /= train_batches\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Mode évaluation\n",
    "        unified_model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        # Boucle sur les batches de validation\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_dataloader, desc=f\"Époque {epoch+1}/{num_epochs} (validation)\"):\n",
    "                # Déplacer les données sur l'appareil approprié\n",
    "                features = batch[\"features\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                task_id = batch[\"task_id\"].to(device)\n",
    "                \n",
    "                # Propagation avant\n",
    "                outputs = unified_model(features, task_id)\n",
    "                \n",
    "                # Calculer la perte\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Accumuler la perte\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "        \n",
    "        # Calculer la perte moyenne pour l'époque\n",
    "        epoch_val_loss /= val_batches\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Mettre à jour le scheduler\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Afficher les pertes\n",
    "        print(f\"Époque {epoch+1}/{num_epochs} - \"\n",
    "              f\"Perte d'entraînement: {epoch_train_loss:.6f}, \"\n",
    "              f\"Perte de validation: {epoch_val_loss:.6f}\")\n",
    "    \n",
    "    return unified_model, train_losses, val_losses\n",
    "\n",
    "# Fonction pour fine-tuner le modèle unifié (dégeler certaines couches)\n",
    "def fine_tune_unified_model(unified_model, train_dataloader, val_dataloader, \n",
    "                           num_epochs=5, learning_rate=5e-5, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Fine-tune le modèle unifié en dégelant certaines couches des modèles pré-entraînés.\n",
    "    \n",
    "    Args:\n",
    "        unified_model: Le modèle unifié\n",
    "        train_dataloader: DataLoader pour les données d'entraînement\n",
    "        val_dataloader: DataLoader pour les données de validation\n",
    "        num_epochs: Nombre d'époques d'entraînement\n",
    "        learning_rate: Taux d'apprentissage\n",
    "        device: Appareil sur lequel effectuer l'entraînement ('cuda' ou 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Le modèle fine-tuné et les historiques de perte\n",
    "    \"\"\"\n",
    "    # Dégeler les dernières couches des modèles pré-entraînés\n",
    "    for name, param in unified_model.transfer_model.named_parameters():\n",
    "        if \"layer\" in name and any(f\"layer.{i}\" in name for i in [10, 11]):  # Dégeler les 2 dernières couches\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    for name, param in unified_model.insertion_model.named_parameters():\n",
    "        if \"layer\" in name and any(f\"layer.{i}\" in name for i in [10, 11]):  # Dégeler les 2 dernières couches\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Entraîner le modèle avec les couches dégelées\n",
    "    return train_unified_model(unified_model, train_dataloader, val_dataloader, \n",
    "                              num_epochs=num_epochs, learning_rate=learning_rate, device=device)\n",
    "\n",
    "# Fonction principale\n",
    "def main():\n",
    "    # Charger le dataset combiné (à partir du script précédent)\n",
    "    combined_dataset = load_and_prepare_datasets()\n",
    "    \n",
    "    # Créer les datasets PyTorch\n",
    "    if \"train\" in combined_dataset and \"validation\" in combined_dataset:\n",
    "        train_dataset = ALOHADataset(combined_dataset[\"train\"])\n",
    "        val_dataset = ALOHADataset(combined_dataset[\"validation\"])\n",
    "    else:\n",
    "        # Si pas de split validation, créer un à partir du train\n",
    "        full_dataset = ALOHADataset(combined_dataset[next(iter(combined_dataset))])\n",
    "        train_size = int(0.8 * len(full_dataset))\n",
    "        val_size = len(full_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Créer les dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Charger les modèles pré-entraînés\n",
    "    transfer_model, insertion_model = load_pretrained_models()\n",
    "    \n",
    "    # Créer le modèle unifié\n",
    "    unified_model = UnifiedALOHAModel(transfer_model, insertion_model)\n",
    "    \n",
    "    # Déterminer l'appareil à utiliser (GPU ou CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Utilisation de l'appareil: {device}\")\n",
    "    \n",
    "    # Entraîner le modèle unifié\n",
    "    print(\"\\nEntraînement du modèle unifié...\")\n",
    "    unified_model, train_losses, val_losses = train_unified_model(\n",
    "        unified_model, train_dataloader, val_dataloader, \n",
    "        num_epochs=10, learning_rate=1e-4, device=device\n",
    "    )\n",
    "    \n",
    "    # Fine-tuner le modèle unifié\n",
    "    print(\"\\nFine-tuning du modèle unifié...\")\n",
    "    unified_model, ft_train_losses, ft_val_losses = fine_tune_unified_model(\n",
    "        unified_model, train_dataloader, val_dataloader, \n",
    "        num_epochs=5, learning_rate=5e-5, device=device\n",
    "    )\n",
    "    \n",
    "    # Tracer les courbes de perte\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Courbes de perte pour l'entraînement initial\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.title('Pertes pendant l\\'entraînement initial')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Courbes de perte pour le fine-tuning\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(ft_train_losses, label='Train')\n",
    "    plt.plot(ft_val_losses, label='Validation')\n",
    "    plt.title('Pertes pendant le fine-tuning')\n",
    "    plt.xlabel('Époque')\n",
    "    plt.ylabel('Perte')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_losses.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder le modèle unifié\n",
    "    model_save_path = \"unified_aloha_model\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    torch.save(unified_model.state_dict(), os.path.join(model_save_path, \"unified_model.pt\"))\n",
    "    print(f\"Modèle unifié sauvegardé dans {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "58fb220133d63b69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset pour la tâche de transfert...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "defda37782de4e3eb3717b85bebcf866"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8367aac83b8422c929049c3896fa6ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du dataset pour la tâche d'insertion...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4718387bd6354627b073636802cd7fa6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/50 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "171194b61d144843a8d57e4fd2b26a5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset transfert: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n",
      "Dataset insertion: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.done', 'index', 'task_index'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n",
      "\n",
      "Exemple d'échantillon de transfert:\n",
      "observation.state: Type <class 'list'>, Forme (14,)\n",
      "action: Type <class 'list'>, Forme (14,)\n",
      "episode_index: 0\n",
      "frame_index: 0\n",
      "timestamp: 0.0\n",
      "next.done: False\n",
      "index: 0\n",
      "task_index: 0\n",
      "\n",
      "Exemple d'échantillon d'insertion:\n",
      "observation.state: Type <class 'list'>, Forme (14,)\n",
      "action: Type <class 'list'>, Forme (14,)\n",
      "episode_index: 0\n",
      "frame_index: 0\n",
      "timestamp: 0.0\n",
      "next.done: False\n",
      "index: 0\n",
      "task_index: 0\n",
      "\n",
      "Ajout des tags aux datasets...\n",
      "Concaténation des datasets...\n",
      "\n",
      "Dataset combiné:\n",
      "train: 45000 échantillons\n",
      "Chargement du modèle pour la tâche de transfert...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in /Users/louloute/PycharmProjects/INF8225_projet/Models/transfer. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 421\u001B[0m\n\u001B[1;32m    418\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModèle unifié sauvegardé dans \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_save_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 421\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[39], line 366\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    363\u001B[0m val_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m    365\u001B[0m \u001B[38;5;66;03m# Charger les modèles pré-entraînés\u001B[39;00m\n\u001B[0;32m--> 366\u001B[0m transfer_model, insertion_model \u001B[38;5;241m=\u001B[39m load_pretrained_models()\n\u001B[1;32m    368\u001B[0m \u001B[38;5;66;03m# Créer le modèle unifié\u001B[39;00m\n\u001B[1;32m    369\u001B[0m unified_model \u001B[38;5;241m=\u001B[39m UnifiedALOHAModel(transfer_model, insertion_model)\n",
      "Cell \u001B[0;32mIn[39], line 22\u001B[0m, in \u001B[0;36mload_pretrained_models\u001B[0;34m()\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChargement du modèle pour la tâche de transfert...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     21\u001B[0m transfer_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/louloute/PycharmProjects/INF8225_projet/Models/transfer/model.safetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 22\u001B[0m transfer_model \u001B[38;5;241m=\u001B[39m AutoModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     23\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(transfer_model_path),\n\u001B[1;32m     24\u001B[0m     local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     25\u001B[0m )\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChargement du modèle pour la tâche d\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minsertion...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m insertion_model_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/louloute/PycharmProjects/INF8225_projet/Models/insertion/model.safetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:531\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     _ \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 531\u001B[0m config, kwargs \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    532\u001B[0m     pretrained_model_name_or_path,\n\u001B[1;32m    533\u001B[0m     return_unused_kwargs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    534\u001B[0m     trust_remote_code\u001B[38;5;241m=\u001B[39mtrust_remote_code,\n\u001B[1;32m    535\u001B[0m     code_revision\u001B[38;5;241m=\u001B[39mcode_revision,\n\u001B[1;32m    536\u001B[0m     _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs,\n\u001B[1;32m    538\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    539\u001B[0m )\n\u001B[1;32m    541\u001B[0m \u001B[38;5;66;03m# if torch_dtype=auto was passed here, ensure to pass it on\u001B[39;00m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs_orig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch_dtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1133\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m pattern \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(pretrained_model_name_or_path):\n\u001B[1;32m   1131\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m CONFIG_MAPPING[pattern]\u001B[38;5;241m.\u001B[39mfrom_dict(config_dict, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwargs)\n\u001B[0;32m-> 1133\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1134\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized model in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpretrained_model_name_or_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1135\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould have a `model_type` key in its \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCONFIG_NAME\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, or contain one of the following strings \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1136\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min its name: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(CONFIG_MAPPING\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1137\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Unrecognized model in /Users/louloute/PycharmProjects/INF8225_projet/Models/transfer. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, colpali, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, depth_pro, deta, detr, diffllama, dinat, dinov2, dinov2_with_registers, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, emu3, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, git, glm, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hiera, hubert, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mistral3, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_vl, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip_vision_model, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, textnet, time_series_transformer, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zamba2, zoedepth"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T23:48:24.719031Z",
     "start_time": "2025-04-14T23:48:24.715837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "from transformers import AutoModel, AutoConfig, PreTrainedModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import wandb  # Pour le suivi des expériences (optionnel)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fonction de chargement des modèles pré-entraînés pour les deux tâches\n",
    "def load_pretrained_models():\n",
    "    \"\"\"\n",
    "    Charge les deux modèles pré-entraînés: un pour le transfert et un pour l'insertion\n",
    "    à partir des chemins locaux.\n",
    "    \"\"\"\n",
    "    print(\"Chargement du modèle pour la tâche de transfert...\")\n",
    "    transfer_model_path = \"/Users/louloute/PycharmProjects/INF8225_projet/Models/transfer/model.safetensors\"\n",
    "    transfer_model = AutoModel.from_pretrained(\n",
    "        os.path.dirname(transfer_model_path),\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    print(\"Chargement du modèle pour la tâche d'insertion...\")\n",
    "    insertion_model_path = \"/Users/louloute/PycharmProjects/INF8225_projet/Models/insertion/model.safetensors\"\n",
    "    insertion_model = AutoModel.from_pretrained(\n",
    "        os.path.dirname(insertion_model_path),\n",
    "        local_files_only=True\n",
    "    )\n",
    "    \n",
    "    # Afficher l'architecture des modèles\n",
    "    print(\"\\nArchitecture du modèle de transfert:\")\n",
    "    print(transfer_model)\n",
    "    \n",
    "    print(\"\\nArchitecture du modèle d'insertion:\")\n",
    "    print(insertion_model)\n",
    "    \n",
    "    # Examiner les paramètres des modèles\n",
    "    transfer_params = sum(p.numel() for p in transfer_model.parameters())\n",
    "    insertion_params = sum(p.numel() for p in insertion_model.parameters())\n",
    "    \n",
    "    print(f\"\\nNombre de paramètres du modèle de transfert: {transfer_params:,}\")\n",
    "    print(f\"Nombre de paramètres du modèle d'insertion: {insertion_params:,}\")\n",
    "    \n",
    "    return transfer_model, insertion_model"
   ],
   "id": "624ffc882932aebe",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5964618e80f3f192"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
